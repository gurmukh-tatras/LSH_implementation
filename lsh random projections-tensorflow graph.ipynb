{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### incorporating multiple tables\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits, load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "class lsh:\n",
    "    def __init__(self, hash_size, data_dim, num_tables,random_type=None):\n",
    "        self.num_rand_vec = hash_size  # number of buckets will be 2**hash_size eg: 2**2=4 (00,01,10,11)\n",
    "        self.dim = data_dim\n",
    "        self.num_tables = num_tables\n",
    "        self.hash_tables = [{} for _ in range(self.num_tables)]\n",
    "        self.seeds = [i for i in range(self.num_tables)]\n",
    "        self.random_vectors = []\n",
    "        for seed in self.seeds:\n",
    "            np.random.seed(seed)\n",
    "            if random_type:\n",
    "                self.random_vectors.append(self.gen_random_vectors(random_type='normal_gpu'))\n",
    "            else:  # normal distribution by default\n",
    "                self.random_vectors.append(self.gen_random_vectors())\n",
    "        print('TENSORFLOW GPU $$$$$$$$$$$$$ available GPU is -->>,',tf.test.gpu_device_name())\n",
    "\n",
    "    def gen_random_vectors(self,random_type=None):\n",
    "        if random_type is None:\n",
    "            # sample from random_normal distribution by default\n",
    "            return np.random.randn(self.num_rand_vec, self.dim)\n",
    "        if random_type == 'normal_gpu':\n",
    "            return tf.random.normal((self.num_rand_vec, self.dim)).numpy()\n",
    "        if random_type == 'uniform':\n",
    "            return np.random.rand(self.num_rand_vec, self.dim)\n",
    "\n",
    "#     @tf.function\n",
    "    def make_hash_key(self, inp):\n",
    "        return ''.join(inp)\n",
    "\n",
    "    def fit(self,data,label):\n",
    "        assert data.shape[1] == self.dim, 'dimension of input data is {} and dimension in LSH object is {}'.format(\n",
    "            data.shape, self.dim)\n",
    "        # sess = tf.Session()\n",
    "        # sess = tf.compat.v1.InteractiveSession()\n",
    "        print('fitting')\n",
    "        gpu_availability = tf.test.is_gpu_available()\n",
    "        print('is GPU availabale ->',gpu_availability)\n",
    "#         if gpu_availability:\n",
    "        session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "#         with tf.device(\"/GPU:0\"):\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "#         print('creating session on gpu, eager execution disabled ')\n",
    "        data_ph = tf.compat.v1.placeholder(\"float\", [None, self.dim])\n",
    "        rand_hash_vec_ph = tf.compat.v1.placeholder(\"float\", [None, self.dim])\n",
    "        distance_matrix_ = tf.matmul(data_ph, tf.transpose(rand_hash_vec_ph))\n",
    "        euclidean_dist_ = tf.sqrt(tf.reduce_sum(distance_matrix_ ** 2, axis=1))\n",
    "        # data_ph = tf.convert_to_tensor(data,dtype=tf.float32)\n",
    "        distance_as_keys = tf.strings.as_string(tf.cast((distance_matrix_ > 0),tf.int8))\n",
    "        keys_ = tf.strings.reduce_join(distance_as_keys,axis=1) # [b'1001101',b'11001011',....]\n",
    "        unique_keys_,idx = tf.unique(keys_)\n",
    "        sorted_distance_idx = tf.argsort(euclidean_dist_)\n",
    "#         (distance, bucket_key, data)\n",
    "        \n",
    "        \n",
    "#         tf.range(0,self.num_tables)\n",
    "        for rand_vec, hash_table in zip(self.random_vectors, self.hash_tables):\n",
    "            # rand_hash_vec_ph = rand_vec\n",
    "            # distance_matrix_ = tf.matmul(data_ph, tf.transpose(rand_hash_vec_ph))\n",
    "            # euclidean_dist_ = tf.sqrt(tf.reduce_sum(distance_matrix_ ** 2, axis=1))\n",
    "            t1 = time.time()\n",
    "            distance_matrix,euclidean_dist,keys, unique_keys, sorted_dist_idx = session.run([distance_matrix_, euclidean_dist_,keys_,\n",
    "                                                                            unique_keys_,sorted_distance_idx],\n",
    "                                                      feed_dict={data_ph: data,\n",
    "                                                                 rand_hash_vec_ph: rand_vec})\n",
    "            print(type(distance_matrix),'type(distance_matrix)',distance_matrix.shape)\n",
    "            print('total keys',len(keys))\n",
    "            print('unique_keys', len(unique_keys))\n",
    "            print('time taken for matmul ', time.time() - t1)\n",
    "            print(sorted_dist_idx,len(sorted_dist_idx))\n",
    "            # distance_matrix, euclidean_dist = np.array(distance_matrix_), np.array(euclidean_dist_)\n",
    "#             keys = list(map(self.make_hash_key, (distance_matrix > 0).astype('int').astype('str')))\n",
    "            # print('euclidean dist', (distance_matrix > 0).astype('int'))\n",
    "\n",
    "            # the keys contain string of length=hash_size (2 bits or 3 bits..) for each document.\n",
    "            # Eg '00','01','10','11'  for hash_size of 2 bits.\n",
    "#             unique_keys = set(keys)\n",
    "            for key in unique_keys:\n",
    "                hash_table[key] = []\n",
    "            \n",
    "            assert len(keys) == len(euclidean_dist), 'shape of euclidean dist matrix is {} and length of keys list'\\\n",
    "                                                     ' is {}. They must match'.format(len(euclidean_dist), len(keys))\n",
    "#             sorted_keys = [(dist,key,key_idx) for dist, key, key_idx in\n",
    "#                            sorted(zip(list(euclidean_dist),keys,range(len(keys))),  key=lambda pair: pair[0])]\n",
    "            # key_idx represents the document index in original data. We need to preserve this info before sorting\n",
    "            # the points in one bucket based on their distances from randomly projected vectors.\n",
    "            for idx in sorted_dist_idx:\n",
    "#                 print(keys[idx],euclidean_dist[idx], data[idx])\n",
    "                hash_table[keys[idx]].append((euclidean_dist[idx], data[idx],label[idx]))\n",
    "        \n",
    "#             for key in sorted_keys:  # (dist,key,key_idx)\n",
    "#                 hash_table[key[1]].append((key[0],key[2]))  # (distance, doc_idx) <<<<<<<< very important\n",
    "                # each key:value pair in hash table looks like this\n",
    "                # '01':[(euclidean_dist, document_idx_in_data), () ,() , ......]\n",
    "                # '01' is a bucket, in which a document might be present in.\n",
    "#         else:\n",
    "#             print('fitting skipped')\n",
    "#             print(hash_table.keys())\n",
    "        return 'success'\n",
    "\n",
    "    def sort_buckets_elements(self):\n",
    "        \"\"\"\n",
    "        call this after function after dividing the data into several buckets.\n",
    "        Idea is to sort the bucket elements based on distance from random projection vector. Given a query point,\n",
    "        we can find the top k similar elements by doing exhaustive search in the bucket. But if the bucket elements\n",
    "        are sorted we can find top k similar items without having to compare all the\n",
    "        elements in a particular bucket. This will speedup query.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def hash_table(self):\n",
    "        return self.hash_tables\n",
    "\n",
    "    def hash_table_dist(self):\n",
    "        distributions = []\n",
    "        for hash_table in self.hash_tables:\n",
    "            summary_of_table = {}\n",
    "            for k, v in hash_table.items():\n",
    "                summary_of_table[k] = len(v)\n",
    "            distributions.append(summary_of_table)\n",
    "        return distributions\n",
    "\n",
    "    def query(self, query_data):\n",
    "        key_for_each_table = []\n",
    "        for rand_vec in self.random_vectors:\n",
    "            key = ''.join((np.dot(query_data, rand_vec.T) > 0).astype('int').astype('str'))\n",
    "            key_for_each_table.append(key)  # each point will be assigned to exactly on bucket in one hash table\n",
    "        result = []\n",
    "        assert len(key_for_each_table) == len(self.hash_tables), 'Some data point is not assigned to any bucket in a hash table'\n",
    "        for hash_table, key in zip(self.hash_tables, key_for_each_table):\n",
    "            if key in hash_table.keys():\n",
    "                result.extend(hash_table[key])\n",
    "        #         print(keys_for_each_table)\n",
    "        return list(set(result))\n",
    "    \n",
    "    def fast_query(self, query_data):\n",
    "        if len(query_data.shape) == 1:\n",
    "            query_data = query_data.reshape((1,-1))\n",
    "        euc_dist_with_key_for_each_table = []\n",
    "        for rand_vec in self.random_vectors:\n",
    "            distance_matrix = np.dot(query_data, rand_vec.T)\n",
    "            euclidean_dist = np.sqrt(np.sum(distance_matrix ** 2, axis=1))\n",
    "            key = list(map(self.make_hash_key, (distance_matrix > 0).astype('int').astype('str')))\n",
    "#             print(key[0].encode('utf-8'),'key<<')\n",
    "            euc_dist_with_key_for_each_table.append((euclidean_dist[0],key[0].encode('utf-8')))\n",
    "            # each point will be assigned to exactly on bucket in one hash table.\n",
    "            # euc_dist_with_key_for_each_table is a list of tuple-->\n",
    "            # [(key in hash table, euclidean distance from rand vector), (), ...]\n",
    "        result = []\n",
    "        assert len(euc_dist_with_key_for_each_table) == len(self.hash_tables), 'Some data point is not assigned to any bucket in a hash table'\n",
    "        for hash_table, distance_key_tuple, in zip(self.hash_tables, euc_dist_with_key_for_each_table):\n",
    "            # print('<<<<<<<<<<<',euc_dist_with_key_for_each_table)\n",
    "            distance = distance_key_tuple[0]\n",
    "            # print('query_distance', distance)\n",
    "            key = distance_key_tuple[1]\n",
    "            if key in hash_table.keys():\n",
    "                # now use the key( or bucket) of each hash table, along with euclidean distance from random projection\n",
    "                # vectors to find the most similar items to the query doc, instead of exhaustive search.\n",
    "                # Using binary search on distances.\n",
    "                bucket_elements = hash_table[key]\n",
    "                # print('bucket_elements', bucket_elements)\n",
    "                candidates = self.binary_search(arr=bucket_elements,query_distance=distance,max_k=5)\n",
    "                # print('candidates',candidates)\n",
    "                result.extend(candidates)\n",
    "        result_docs,labels = np.vstack([tupl[1] for tupl in result]), [tupl[2] for tupl in result]\n",
    "#         print(result_docs,labels,',,,,,,,')\n",
    "        return result_docs,labels\n",
    "\n",
    "#     def fast_query(self, query_data):\n",
    "#         if len(query_data.shape) == 1:\n",
    "#             query_data = query_data.reshape((1,-1))\n",
    "#         euc_dist_with_key_for_each_table = []\n",
    "#         session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "#         tf.compat.v1.disable_eager_execution()\n",
    "#         data_ph = tf.compat.v1.placeholder(\"float\", [None, self.dim])\n",
    "#         rand_hash_vec_ph = tf.compat.v1.placeholder(\"float\", [None, self.dim])\n",
    "#         distance_matrix_ = tf.matmul(data_ph, tf.transpose(rand_hash_vec_ph))\n",
    "#         euclidean_dist_ = tf.sqrt(tf.reduce_sum(distance_matrix_ ** 2, axis=1))\n",
    "#         # data_ph = tf.convert_to_tensor(data,dtype=tf.float32)\n",
    "#         distance_as_keys = tf.strings.as_string(tf.cast((distance_matrix_ > 0),tf.int8))\n",
    "#         keys_ = tf.strings.reduce_join(distance_as_keys,axis=1) # [b'1001101',b'11001011',....]\n",
    "# #         tf.while_loop(i>0)\n",
    "#         for rand_vec in self.random_vectors:\n",
    "#             distance_matrix,euclidean_dist,keys = session.run([distance_matrix_, euclidean_dist_,keys_],\n",
    "#                                                       feed_dict={data_ph: query_data,\n",
    "#                                                                  rand_hash_vec_ph: rand_vec})\n",
    "# #             distance_matrix = np.dot(query_data, rand_vec.T)\n",
    "# #             euclidean_dist = np.sqrt(np.sum(distance_matrix ** 2, axis=1))\n",
    "# #             key = list(map(self.make_hash_key, (distance_matrix > 0).astype('int').astype('str')))\n",
    "#             print(type(distance_matrix),'type(distance_matrix)',distance_matrix.shape)\n",
    "#             print('total keys',len(keys))\n",
    "#             euc_dist_with_key_for_each_table.append((euclidean_dist,keys))\n",
    "#             # each point will be assigned to exactly on bucket in one hash table.\n",
    "#             # euc_dist_with_key_for_each_table is a list of tuple-->\n",
    "#             # [(key in hash table, euclidean distance from rand vector), (), ...]\n",
    "# #         print(euc_dist_with_key_for_each_table,'euc_dist_with_key_for_each_table', len(euc_dist_with_key_for_each_table))\n",
    "# #         print(len(euc_dist_with_key_for_each_table[0]))\n",
    "#         result = []\n",
    "#         assert len(euc_dist_with_key_for_each_table) == len(self.hash_tables), 'Some data point is not assigned to any bucket in a hash table'\n",
    "#         for hash_table, distance_key_tuple, in zip(self.hash_tables, euc_dist_with_key_for_each_table):\n",
    "#             # print('<<<<<<<<<<<',euc_dist_with_key_for_each_table)\n",
    "            \n",
    "#             for dist_key in distance_key_tuple:\n",
    "#                 print('dist_key', dist_key)\n",
    "#                 key = dist_key[1]\n",
    "#                 distance = dist_key[0]\n",
    "#                 print('key and bucket', key, distance)\n",
    "#                 if key in hash_table.keys():\n",
    "#                     # now use the key( or bucket) of each hash table, along with euclidean distance from random projection\n",
    "#                     # vectors to find the most similar items to the query doc, instead of exhaustive search.\n",
    "#                     # Using binary search on distances.\n",
    "#                     bucket_elements = hash_table[key]\n",
    "#                     # print('bucket_elements', bucket_elements)\n",
    "#                     candidates = self.binary_search(arr=bucket_elements,query_distance=distance,max_k=5)\n",
    "#                     # print('candidates',candidates)\n",
    "#                     result.extend(candidates)\n",
    "# #         print(len(result),'this much candidates')\n",
    "# #         result_doc_idexes = [tupl[1] for tupl in result]\n",
    "#         return result\n",
    "\n",
    "    def binary_search(self,arr,query_distance,max_k):\n",
    "        mid = len(arr)//2\n",
    "        if arr[mid][0] > query_distance:\n",
    "            arr = arr[:mid]\n",
    "            if len(arr) <= max_k:\n",
    "                return arr\n",
    "            else:\n",
    "                return self.binary_search(arr, query_distance, max_k)\n",
    "        if arr[mid][0] < query_distance:\n",
    "            arr = arr[mid:]\n",
    "            if len(arr) <= max_k:\n",
    "                return arr\n",
    "            else:\n",
    "                return self.binary_search(arr,query_distance,max_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from LSH import lsh as lsh_\n",
    "# from LSH_TF import lsh as lsh_tf\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import nearpy\n",
    "\n",
    "def load_mnist_data(num_samples,test_size=0.1):\n",
    "    print('loading data')\n",
    "    threshold = 70000\n",
    "    if num_samples > threshold:\n",
    "        num_iters = num_samples//threshold\n",
    "        df_list = []\n",
    "        for _ in range(num_iters):\n",
    "            mnist = pd.read_csv('mnist-in-csv/mnist_train.csv',nrows=threshold)\n",
    "            df_list.append((mnist))\n",
    "        mnist = pd.concat(df_list)\n",
    "\n",
    "    else:\n",
    "        mnist = pd.read_csv('mnist-in-csv/mnist_train.csv', nrows=num_samples)\n",
    "    mnist_data = np.array(mnist[mnist.columns[1:]])\n",
    "    mnist_labels = np.array(mnist[mnist.columns[0]])\n",
    "    X_train, X_test, y_train, y_test = split_data(x=mnist_data, y=mnist_labels, test_size=test_size)\n",
    "    # X_train = X_train / 255\n",
    "    # X_test = X_test / 255\n",
    "    print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def split_data(x, y, test_size):\n",
    "    size = len(x)\n",
    "    train_idx_stop = int(size * (1 - test_size))\n",
    "    test_idx_start = train_idx_stop\n",
    "    test_idx_stop = train_idx_stop + int(size * test_size)\n",
    "    x_train = x[0:train_idx_stop]\n",
    "    x_test = y[0:train_idx_stop]\n",
    "    y_train = x[test_idx_start:test_idx_stop]\n",
    "    y_test = y[test_idx_start:test_idx_stop]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def exhaustive_search(query_doc,candidates,labels):\n",
    "    # print(len(candidates),'length of candidates')\n",
    "    sims = cosine_similarity(candidates,query_doc)\n",
    "    max_sim_idx = sims.argmax()\n",
    "    return candidates[max_sim_idx], labels[max_sim_idx]\n",
    "\n",
    "\n",
    "def candidate_label_distribution(candidates,y_train):\n",
    "    candidate_label_dist = {i: 0 for i in range(10)}\n",
    "    for doc_tuple in candidates:\n",
    "        # candidates is list of tuple, each tuple contains distance from random projection vector and doc index\n",
    "        candidate_label_dist[y_train[doc_tuple[1]]] += 1\n",
    "    return candidate_label_dist\n",
    "\n",
    "\n",
    "def check_accuracy(x_test, y_test,X_train,y_train, lsh):\n",
    "    acc = 0\n",
    "    for x,y in zip(x_test,y_test):\n",
    "        candidate_docs,labels = lsh.fast_query(x)\n",
    "        if len(candidate_docs) > 0:\n",
    "            most_similar_doc,prediction = exhaustive_search(query_doc=x.reshape(1, -1),\n",
    "                                                            candidates=candidate_docs, labels=labels)\n",
    "            if prediction == y:\n",
    "                acc += 1\n",
    "    return acc/len(x_test)\n",
    "\n",
    "def check_accuracy_tf(x_test, y_test,X_train,y_train, lsh):\n",
    "    acc = 0\n",
    "#     print('number of test docs',x_test.shape)\n",
    "#     for x,y in zip(x_test,y_test):\n",
    "        # print('>>>>',x,y)\n",
    "    fast_query_candidates = lsh.fast_query(x_test)\n",
    "#     print(len(fast_query_candidates), 'fast_query_candidates length')\n",
    "#     print(fast_query_candidates[0], )\n",
    "#     candidate_docs = X_train[fast_query_candidates]\n",
    "    if len(candidate_docs) > 0:\n",
    "        most_similar_arg_max = exhaustive_search(query_doc=x.reshape(1, -1), candidates=candidate_docs)\n",
    "        most_similar_doc_idx = fast_query_candidates[most_similar_arg_max]\n",
    "        prediction = y_train[most_similar_doc_idx]\n",
    "        # print(prediction,y)\n",
    "        if prediction == y:\n",
    "            acc += 1\n",
    "    return acc/len(x_test)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X_train, y_train, X_test, y_test = load_mnist_data(num_samples=3000,test_size=0.1)\n",
    "    lsh = lsh_(hash_size=12, data_dim=X_train.shape[1], num_tables=10)\n",
    "\n",
    "    t1 = time.time()\n",
    "    lsh.fit(X_train)\n",
    "    print('total time taken for fitting', time.time() - t1)\n",
    "    rand_doc_idx = 12\n",
    "    query = X_test[rand_doc_idx]\n",
    "    print('label of query doc ', y_test[rand_doc_idx])\n",
    "\n",
    "    t1 = time.time()\n",
    "    candidates = (lsh.query(query))\n",
    "    print('total time taken for prediction', time.time() - t1)\n",
    "    print('number of candidates:', len(candidates))\n",
    "    print('candidate label distribution:', candidate_label_distribution(candidates,y_train))\n",
    "\n",
    "    print('hash tables dist', lsh.hash_table_dist())\n",
    "    t1 = time.time()\n",
    "    fast_query_candidates = lsh.fast_query(query)\n",
    "    print('total time taken for prediction using fast_query', time.time() - t1)\n",
    "    candidate_label_dist = {i: 0 for i in range(10)}\n",
    "    for doc_idx in fast_query_candidates:\n",
    "        # candidates is list of tuple, each tuple contains distance from random projection vector and doc index\n",
    "        candidate_label_dist[y_train[doc_idx]] += 1\n",
    "    print('candidate label distribution:',candidate_label_dist )\n",
    "    print('fast_query_candidates ',fast_query_candidates)\n",
    "    print('fast_query_candidates length',len(fast_query_candidates))\n",
    "    candidate_docs = X_train[fast_query_candidates]\n",
    "    most_similar_arg_max = exhaustive_search(query_doc=query.reshape(1,-1), candidates=candidate_docs)\n",
    "    most_similar_doc_idx = fast_query_candidates[most_similar_arg_max]\n",
    "    print('label for the query is ', y_train[most_similar_doc_idx], 'idx', most_similar_doc_idx)\n",
    "\n",
    "\n",
    "def main_2():\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data(num_samples=7000, test_size=0.1)\n",
    "    lsh = lsh_(hash_size=8, data_dim=x_train.shape[1], num_tables=5)\n",
    "    t1 = time.time()\n",
    "    lsh.fit(x_train)\n",
    "    print('total time taken for fitting', time.time() - t1)\n",
    "    t1 = time.time()\n",
    "    accuracy = check_accuracy(x_test, y_test,x_train,y_train,lsh)\n",
    "    print('accuracy is ', accuracy)\n",
    "    print('total time taken for checking accuracy for {} docs is'.format(len(x_test)), time.time() - t1)\n",
    "\n",
    "\n",
    "def main_tf():\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data(num_samples=7000, test_size=0.1)\n",
    "    lsh_ = lsh(hash_size=8, data_dim=x_train.shape[1], num_tables=5,random_type=None)\n",
    "    t1 = time.time()\n",
    "    lsh_.fit(x_train,y_train)\n",
    "    print('total time taken for fitting using tensorflow', time.time() - t1)\n",
    "    t1 = time.time()\n",
    "    accuracy = check_accuracy(x_test, y_test,x_train,y_train,lsh_)\n",
    "    print('accuracy is ', accuracy)\n",
    "    print('total time taken for checking accuracy for {} docs is'.format(len(x_test)), time.time() - t1)\n",
    "#     print(lsh_.hash_table_dist())\n",
    "\n",
    "# main_2()\n",
    "# main_tf()\n",
    "#\n",
    "\n",
    "# tf.test.is_gpu_available()\n",
    "# tf.test.gpu_device_name()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# from nearpy import Engine\n",
    "# from nearpy.hashes import RandomBinaryProjections\n",
    "#\n",
    "# # Dimension of our vector space\n",
    "# dimension = 784\n",
    "#\n",
    "# # Create a random binary hash with 10 bits\n",
    "# rbp = RandomBinaryProjections('rbp', 10)\n",
    "#\n",
    "# # Create engine with pipeline configuration\n",
    "# engine = Engine(dimension, lshashes=[rbp])\n",
    "#\n",
    "# # Index 1000000 random vectors (set their data to a unique string)\n",
    "# for img_vec in X_train:\n",
    "#     engine.store_vector(img_vec, 'data_%d' % index)\n",
    "#\n",
    "# # Create random query vector\n",
    "#\n",
    "# query = X_test[1]\n",
    "# # Get nearest neighbours\n",
    "# t1 = time.time()\n",
    "# N = engine.neighbours(query)\n",
    "# # check_accuracy(X_test, y_test,X_train,y_train)\n",
    "# print('time taken for query', time.time()-t1)\n",
    "# show_mnist_image(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "(6300, 784) (6300,) (700, 784) (700,)\n",
      "TENSORFLOW GPU $$$$$$$$$$$$$ available GPU is -->>, \n",
      "fitting\n",
      "is GPU availabale -> False\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "<class 'numpy.ndarray'> type(distance_matrix) (6300, 8)\n",
      "total keys 6300\n",
      "unique_keys 163\n",
      "time taken for matmul  0.1644916534423828\n",
      "[4362 2945 2445 ... 1420  554 2796] 6300\n",
      "<class 'numpy.ndarray'> type(distance_matrix) (6300, 8)\n",
      "total keys 6300\n",
      "unique_keys 170\n",
      "time taken for matmul  0.04644298553466797\n",
      "[4522 1412 1123 ... 1271 1618 5972] 6300\n",
      "<class 'numpy.ndarray'> type(distance_matrix) (6300, 8)\n",
      "total keys 6300\n",
      "unique_keys 156\n",
      "time taken for matmul  0.03956294059753418\n",
      "[4552 2786 1093 ... 6039 5965 6063] 6300\n",
      "<class 'numpy.ndarray'> type(distance_matrix) (6300, 8)\n",
      "total keys 6300\n",
      "unique_keys 238\n",
      "time taken for matmul  0.054404497146606445\n",
      "[3475 5659 3150 ... 6013  944 6139] 6300\n",
      "<class 'numpy.ndarray'> type(distance_matrix) (6300, 8)\n",
      "total keys 6300\n",
      "unique_keys 177\n",
      "time taken for matmul  0.03632998466491699\n",
      "[1850 5112 2424 ... 2796 6139 5972] 6300\n",
      "total time taken for fitting using tensorflow 0.4192359447479248\n",
      "accuracy is  0.7757142857142857\n",
      "total time taken for checking accuracy for 700 docs is 0.9524531364440918\n"
     ]
    }
   ],
   "source": [
    "main_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data.shape[1] == dim, 'dimension of input data is {} and dimension in LSH object is {}'.format(\n",
    "    data.shape, self.dim)\n",
    "# sess = tf.Session()\n",
    "# sess = tf.compat.v1.InteractiveSession()\n",
    "print('fitting')\n",
    "gpu_availability = tf.test.is_gpu_available()\n",
    "print('is GPU availabale ->',gpu_availability)\n",
    "#         if gpu_availability:\n",
    "session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "#         with tf.device(\"/GPU:0\"):\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#         print('creating session on gpu, eager execution disabled ')\n",
    "data_ph = tf.compat.v1.placeholder(\"float\", [None, self.dim])\n",
    "rand_hash_vec_ph = tf.compat.v1.placeholder(\"float\", [None, self.dim])\n",
    "distance_matrix_ = tf.matmul(data_ph, tf.transpose(rand_hash_vec_ph))\n",
    "euclidean_dist_ = tf.sqrt(tf.reduce_sum(distance_matrix_ ** 2, axis=1))\n",
    "# data_ph = tf.convert_to_tensor(data,dtype=tf.float32)\n",
    "\n",
    "distance_matrix,euclidean_dist = session.run([distance_matrix_, euclidean_dist_],\n",
    "                                               feed_dict={data_ph: data,\n",
    "                                                          rand_hash_vec_ph: rand_vec})\n",
    "keys = list(map(self.make_hash_key, (distance_matrix > 0).astype('int').astype('str')))\n",
    "\n",
    "\n",
    "# for rand_vec, hash_table in zip(self.random_vectors, self.hash_tables):\n",
    "#     # rand_hash_vec_ph = rand_vec\n",
    "#     # distance_matrix_ = tf.matmul(data_ph, tf.transpose(rand_hash_vec_ph))\n",
    "#     # euclidean_dist_ = tf.sqrt(tf.reduce_sum(distance_matrix_ ** 2, axis=1))\n",
    "#     t1 = time.time()\n",
    "#     distance_matrix,euclidean_dist = session.run([distance_matrix_, euclidean_dist_],\n",
    "#                                               feed_dict={data_ph: data,\n",
    "#                                                          rand_hash_vec_ph: rand_vec})\n",
    "#     print('time taken for matmul ', time.time() - t1)\n",
    "#     # distance_matrix, euclidean_dist = np.array(distance_matrix_), np.array(euclidean_dist_)\n",
    "#     keys = list(map(self.make_hash_key, (distance_matrix > 0).astype('int').astype('str')))\n",
    "#     # print('euclidean dist', (distance_matrix > 0).astype('int'))\n",
    "\n",
    "#     # the keys contain string of length=hash_size (2 bits or 3 bits..) for each document.\n",
    "#     # Eg '00','01','10','11'  for hash_size of 2 bits.\n",
    "#     unique_keys = set(keys)\n",
    "#     for key in unique_keys:\n",
    "#         hash_table[key] = []\n",
    "#     assert len(keys) == len(euclidean_dist), 'shape of euclidean dist matrix is {} and length of keys list'\\\n",
    "#                                              ' is {}. They must match'.format(len(euclidean_dist), len(keys))\n",
    "#     sorted_keys = [(dist,key,key_idx) for dist, key, key_idx in\n",
    "#                    sorted(zip(list(euclidean_dist),keys,range(len(keys))),  key=lambda pair: pair[0])]\n",
    "#     # key_idx represents the document index in original data. We need to preserve this info before sorting\n",
    "#     # the points in one bucket based on their distances from randomly projected vectors.\n",
    "#     for key in sorted_keys:  # (dist,key,key_idx)\n",
    "#         hash_table[key[1]].append((key[0],key[2]))  # (distance, doc_idx) <<<<<<<< very important\n",
    "#         # each key:value pair in hash table looks like this\n",
    "#         # '01':[(euclidean_dist, document_idx_in_data), () ,() , ......]\n",
    "#         # '01' is a bucket, in which a document might be present in.\n",
    "# #         else:\n",
    "# #             print('fitting skipped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
